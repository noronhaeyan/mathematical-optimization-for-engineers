{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/svtLogo.png\"/>\n",
    "</div>\n",
    "<center><h1>Mathematical Optimization for Engineers</h1></center>\n",
    "<center><h2>Lab 2</h2></center>\n",
    "<center><h2>Basic math</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\mr}[1]{\\mathrm{#1}}\n",
    "\\newcommand{\\D}{\\displaystyle}\n",
    "\\newcommand{\\bm}[1]{\\text{\\mathbf $#1$}}\n",
    "\\newcommand{\\bx}{\\mathbf{ x}}\n",
    "\\newcommand{\\f}{\\mathbf{{f}}}\n",
    "\\newcommand{\\g}{\\mathbf{ g}}\n",
    "\\newcommand{\\h}{\\mathbf{ h}}\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "\\newcommand{\\A}{\\mathbf{ A}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bnabla}{\\mathbf{\\nabla}}\n",
    "$$\n",
    "In this lab, we will learn about Jacobians, gradients and Hessian matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Notation</u>: Please note that throughout the course, we will denote matrices and vectors with boldface letters. Their components will be denoted by normal letters with subscripts. For example,\n",
    "$$\n",
    "\\bx = \\left(\\begin{array}{c}\n",
    "x_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian\n",
    "Let $\\ \\mathbf \\f:\\R^n \\rightarrow \\R^m,\\,\\bx \\mapsto \\f(\\bx)$ be a continuously differentiable function, where\n",
    "$$\n",
    "\\bx = \\left(\\begin{array}{c}\n",
    "x_1 \\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{array} \\right) , \\qquad \\f(\\bx)  = \\left(\\begin{array}{c}\n",
    "f_1(\\bx) \\\\ \n",
    "\\vdots \\\\ \n",
    "f_m(\\bx) \n",
    "\\end{array} \\right).\n",
    "$$\n",
    "The Jacobian $\\f'(\\bx)$ is defined by the matrix\n",
    "$$\n",
    "\\f'(\\bx) = \\left(  \\begin{array}{ccc}\n",
    "\\frac{\\partial f_1(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(\\bx)}{\\partial x_n} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac{\\partial f_m(\\bx)}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(\\bx)}{\\partial x_n}\n",
    "\\end{array}  \\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Now, let $f:\\R^n \\rightarrow \\R,\\,\\bx \\mapsto f(\\bx)$ be a *scalar-valued* continuously differentiable function\n",
    "with vector-valued arguments.\n",
    "The gradient $\\bnabla f(\\bx)$ and Jacobian $f'(\\bx)$ are defined as the column vector and row vector, respectively,\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx)  = \\left(\\begin{array}{c}\n",
    "\t\\frac{\\partial f(\\bx)}{\\partial x_1} \\\\ \n",
    "\t\\vdots \\\\ \n",
    "\t\\frac{\\partial f(\\bx)}{\\partial x_n}\n",
    "\\end{array} \\right), \\qquad  f'(\\bx)  = \\left(\\begin{array}{ccc}  \n",
    "\\frac{\\partial f(\\bx)}{\\partial x_1} & \n",
    "\\cdots & \n",
    "\\frac{\\partial f(\\bx)}{\\partial x_n}\n",
    "\\end{array}\\right).\n",
    "$$\n",
    "\n",
    "Note that  $\\bnabla f(\\bx)^T=f'(\\bx)$.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of the scalar product of two functions\n",
    "\n",
    "Let $\\g,\\h:\\R^n\\rightarrow\\R^n$ be two continuously differentiable functions. We want to compute the\n",
    "gradient of $f:\\R^n\\rightarrow \\R,\\;\\bx \\mapsto f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, where $\\bx\\in\\R^n$.\n",
    "We have\n",
    "\n",
    "$$\n",
    "f(\\bx) = \\g(\\bx)^T\\h(\\bx) = \\sum_{i=1}^{n} g_i(\\bx)h_i(\\bx).\n",
    "$$\n",
    "\n",
    "The derivative with respect to $x_j$ ($1 \\le j \\le n$) is computed by the application of the product rule\n",
    "\n",
    "\\begin{equation}\\label{eq:1}\n",
    "\\frac{\\partial f(\\bx)}{\\partial x_j} = \\sum_{i=1}^{n} \\left(\\frac{\\partial g_i(\\bx)}{\\partial x_j}h_i(\\bx)  +g_i(\\bx)\\frac{\\partial h_i(\\bx)}{\\partial x_j}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "With the notations\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\g(\\bx)}{\\partial x_j}   = \\left(\\begin{array}{c}\n",
    "\\frac{\\partial g_1(\\bx)}{\\partial x_j} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial g_n(\\bx)}{\\partial x_j}\n",
    "\\end{array} \\right) \\quad \\text{and} \\quad \n",
    "\\frac{\\partial \\h(\\bx)}{\\partial x_j}   \n",
    "= \\left(\\begin{array}{c}\n",
    "\\frac{\\partial h_1(\\bx)}{\\partial x_j} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h_n(\\bx)}{\\partial x_j}\n",
    "\\end{array} \\right), \\text{ respectively},\n",
    "$$\n",
    "\n",
    "we can rewrite the equation as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(\\bx)}{\\partial x_j} = \\frac{\\partial \\g(\\bx)}{\\partial x_j} ^T \\h(\\bx) + \n",
    "\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}  = \\h(\\bx)^T\\frac{\\partial \\g(\\bx)}{\\partial x_j}\n",
    "+\\g(\\bx)^T \\frac{\\partial \\h(\\bx)}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "Finally,\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx)^T = f'(\\bx) = \\h(\\bx)^T \\g'(\\bx) + \\g(\\bx)^T\\h'(\\bx).\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\implies \\bnabla f(\\bx) = \\bnabla g(\\bx) \\ \\h(\\bx) + \\bnabla h(\\bx) \\ \\g(\\bx).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivative of quadratic form\n",
    "If $\\A\\in\\R^{n\\times n}$ is a symmetric matrix, the function\n",
    "\n",
    "$$\n",
    "f:\\R^n\\rightarrow \\R,\\quad\\bx\\mapsto f(\\bx) = \\bx^T\\,\\A\\,\\bx\n",
    "$$ is called a quadratic form. <br>\n",
    "<br>\n",
    "With the definitions,\n",
    "\n",
    "$$\n",
    "\\g(\\bx) := \\bx \\ \\text{and } \\ \\h(\\bx):= \\A\\,\\bx,\n",
    "$$\n",
    "\n",
    "we have $f(\\bx) = \\g(\\bx)^T\\h(\\bx)$, i.e., exactly the situation as above.\n",
    "With $\\g'(\\bx) = \\mathbf{ I}$, where $\\mathbf{ I}$ denotes the unity matrix, and $\\h'(\\bx) = \\A$, it is more or less easy to see that the gradient of $f$ is given by\n",
    "\n",
    "$$\n",
    "\\bnabla f(\\bx) = 2\\,\\A\\,\\bx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "Let the function $f:\\R^2 \\rightarrow \\R$ be defined as\n",
    "$ f(x,y) = \\frac{1}{2} (x^2 + \\alpha y^2), \\alpha \\in \\R $\n",
    "\n",
    "<u>Task 1</u>: Find all stationary points of the function $f$.\n",
    "\n",
    "\n",
    "<u>Task 2</u>: Calculate the Hessian of the function $f$ for arbitrary $x$ and $y$.\n",
    "\n",
    "\n",
    "<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$ with respect to $x$ and $y$?\n",
    "\n",
    "\n",
    "<u>Task 4</u>: Characterize the stationary points for positive and negative $\\alpha$.\n",
    "\n",
    "\n",
    "<u>Task 5</u>: Characterize the convexity of the function for every $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Rosenbrock function\n",
    "The Rosenbrock function is a famous test function used in optimization. <br>\n",
    "<br>\n",
    "It is defined by: \n",
    "$f:\\R^2 \\rightarrow \\R$, $ f(x,y) = (a-x)^2 + b(y-x^2)^2 $ with $a=1, b=100$\n",
    "\n",
    "<u>Task 1</u>: Find all stationary points of the function $f$.\n",
    "\n",
    "<u>Task 2</u>: Calculate the Hessian of the function $f$ for the stationary points found. (Hint: use Python)\n",
    "\n",
    "<u>Task 3</u>: What are the eigenvalues of the Hessian of the function $f$? (Hint: use Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x):\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute hessian using autograd\n",
    "import autograd\n",
    "x0 = # stationary point\n",
    "hessian_rosenbrock = # add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the eigenvalues using numpy\n",
    "import numpy as np\n",
    "# add your code here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
