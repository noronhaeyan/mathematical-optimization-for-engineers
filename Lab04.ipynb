{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"figures/svtLogo.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Mathematical Optimization for Engineers</h1></center>\n",
    "<center><h2>Lab 4 - Convergence rates</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the function $f:\\mathcal R^2  \\rightarrow \\mathcal R$ be defined as\n",
    "$$f(x,y) = x^2+y^3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 1</u>: Calculate the first derivative of the function for arbitrary $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 2</u>: Calculate the coordinates of the stationary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 3</u>: Calculate the Hessian of the function for arbitrary $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 4</u>: What are the Eigenvalues of the Hessian for arbitrary $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 5</u>: Check the second-order necessary and the second-order sufficient conditions of the stationary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Task 6</u>: Characterize the stationary point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you have the opportunity to experiment with different solvers for unconstrained optimization.\n",
    "\n",
    "We have implemented the following solvers for you:<br><br>\n",
    "(a) steepest_descent with armijo step-length<br>\n",
    "(b) steepest_descent with wolfe step-length<br>\n",
    "(c) steepest_descent with constant step-length<br>\n",
    "(d) newton's method<br>\n",
    "(e) inexact newton's method with CG (from scipy)<br>\n",
    "(f) CG method (from scipy)<br>\n",
    "(g) bfgs method (from scipy)\n",
    "\n",
    "Moreover, you can test these methods on several different objective functions defined below.\n",
    "\n",
    "Try to answer the following questions:<br>\n",
    "1. What happens when you change the initial point? and why? <br>\n",
    "2. Are the given problems convex or non-convex? <br>\n",
    "3. By looking at the convergence plots, can you identify linear, order-p or super-linear convergence rates?\n",
    "4. Which parameters can you change in different solvers to improve/worsen their respective performances?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "import scipy.optimize as minimize\n",
    "\n",
    "from numpy.linalg import inv\n",
    "\n",
    "from autograd import grad\n",
    "from autograd import hessian\n",
    "from autograd import elementwise_grad as egrad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import matplotlib.ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can ignore this cell. It only serves to create nice plots.\n",
    "def plot_optimization(func, xIter):\n",
    "    \n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    f = []\n",
    "    \n",
    "    x1 = [elx[0] for elx in xIter]\n",
    "    x2 = [ely[1] for ely in xIter]\n",
    "    f = [func([i,j]) for i,j in zip(x1,x2)]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10)) # create figure\n",
    "    ax = fig.add_subplot(111,projection='3d' )\n",
    "    ax.view_init(elev=30, azim=30)\n",
    "    \n",
    "    ax.plot(x1, x2, f, linestyle=\"dashed\",linewidth=1, color=\"red\")\n",
    "\n",
    "    for it in range(len(f)):\n",
    "        ax.scatter(x1[it],x2[it],f[it], marker='x', color=\"red\")\n",
    "    \n",
    "    xmax = max(x1)\n",
    "    xmin = min(x1)\n",
    "    ymax = max(x2)\n",
    "    ymin = min(x2)\n",
    "    \n",
    "    X = np.linspace(xmin-1, xmax+1)\n",
    "    Y = np.linspace(ymin-1, ymax+1)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    \n",
    "    zs = np.array(func([np.ravel(X), np.ravel(Y)]))\n",
    "    Z = zs.reshape(X.shape)\n",
    "    \n",
    "    # Surface plot:\n",
    "    ax.plot_surface(X, Y, Z, cmap=plt.cm.jet, antialiased=True,cstride=1,rstride=1, alpha=0.75)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sixhump(x): #scipy-lectures.org/intro/scipy/auto_examples/plot_2d_minimization.html\n",
    "    return ((4 - 2.1*x[0]**2 + x[0]**4 / 3.) * x[0]**2 + x[0] * x[1] + (-4 + 4*x[1]**2) * x[1] **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen(x): #scipy-lectures.org/intro/scipy/auto_examples/plot_2d_minimization.html\n",
    "    return ((x[0]-1)**2 + 100*(x[1]-x[0]**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(X):\n",
    "    x, y = X[0], X[1]\n",
    "    return ((1.500 - x + x*y)**2 + (2.250 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjiman (X):\n",
    "    x, y = X[0], X[1]\n",
    "    return (np.cos(x) * np.sin(y)) - (x / (y**2.0 + 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(func, x): \n",
    "    return grad(func)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian_function(func,x):\n",
    "    return hessian(func)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, step rule, c1, c2, initial alpha, optimality tolerance)\n",
    "def steepest_descent(function, x0, stepRule, c1, c2, alpha0, tol=0.01):     \n",
    "    \n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))\n",
    "    \n",
    "    # count number of steps taken by method\n",
    "    it = 0\n",
    "    \n",
    "    # accumulate number of iterations needed by linesearch algorithm in every step\n",
    "    ls_iter = 0\n",
    "    \n",
    "    # store iterates for plotting\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)): \n",
    "\n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -1.0 * gradient_function(function,xCur)\n",
    "        \n",
    "        # calculate step-length\n",
    "        if (stepRule=='armijo'):\n",
    "            alpha, ls_ = armijo(function,xCur,direction, c1, alpha0)\n",
    "        elif (stepRule=='wolfe'):\n",
    "            alpha, ls_ = wolfe(function,xCur,direction, c1, c2, alpha0)\n",
    "        else:\n",
    "            alpha = 0.001\n",
    "            ls_ = 1\n",
    "        ls_iter = ls_iter + ls_\n",
    "        \n",
    "        # update current point\n",
    "        xCur = xCur + alpha * direction\n",
    "        gCur = gradient_function(function,xCur)\n",
    "        fcur = function(xCur)\n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (objective function, initial guess, optimality tolerance)\n",
    "def newton_descent(function, x0, tol=0.01): \n",
    "\n",
    "    xCur = x0\n",
    "    fcur = function(xCur)\n",
    "    gCur = gradient_function(function,xCur)\n",
    "    hCur = hessian_function(function,xCur)    \n",
    "    \n",
    "    # norm of the gradient at initial point for optimality condition\n",
    "    nmg0 = np.linalg.norm(gradient_function(function, x0))    \n",
    "    \n",
    "    # count number of steps taken by method    \n",
    "    it = 0\n",
    "    \n",
    "    # store iterates for plotting    \n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    while(np.linalg.norm(gCur)>tol*min(1,nmg0)):\n",
    "        \n",
    "        it=it+1\n",
    "        \n",
    "        # calculate descent direction\n",
    "        direction = -1.0 * np.dot(inv(hCur),gCur)\n",
    "        \n",
    "        # calculate step-length\n",
    "        alpha = 1.0\n",
    "        \n",
    "        # update current point\n",
    "        xCur = xCur + alpha * direction\n",
    "        fcur = function(xCur)        \n",
    "        gCur = gradient_function(function,xCur)\n",
    "        hCur = hessian_function(function,xCur)\n",
    "        xIter.append(xCur)\n",
    "\n",
    "    return xCur, fcur, it, xIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS (scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs_scipy(func, x0):\n",
    "\n",
    "    global Nfeval\n",
    "    global xIter    \n",
    "    Nfeval = 0\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    def callbackF(Xi):\n",
    "\n",
    "        global Nfeval\n",
    "        global xIter \n",
    "        Nfeval += 1    \n",
    "        xIter.append(Xi)\n",
    "    \n",
    "    [xopt, fopt, gopt, Bopt, func_calls, grad_calls, warnflg] = \\\n",
    "        minimize.fmin_bfgs(func, x0, callback=callbackF, maxiter=2000, full_output=True, disp=False, retall=False)\n",
    "    \n",
    "    return xopt, fopt, Nfeval, xIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CG (scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cg_scipy(func, x0):\n",
    "\n",
    "    global Nfeval\n",
    "    global xIter    \n",
    "    Nfeval = 0\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    def callbackF(Xi):\n",
    "\n",
    "        global Nfeval\n",
    "        global xIter \n",
    "        Nfeval += 1    \n",
    "        xIter.append(Xi)\n",
    "    \n",
    "    [xopt, fopt, func_calls, grad_calls, warnflg] = \\\n",
    "        minimize.fmin_cg(func, x0, callback=callbackF, maxiter=2000, full_output=True, disp=False, retall=False)\n",
    "    \n",
    "    return xopt, fopt, Nfeval, xIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton with CG (scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ncg_scipy(func, x0):\n",
    "\n",
    "    global Nfeval\n",
    "    global xIter    \n",
    "    Nfeval = 0\n",
    "    xIter=[]\n",
    "    xIter.append(x0)\n",
    "    \n",
    "    def callbackF(Xi):\n",
    "\n",
    "        global Nfeval\n",
    "        global xIter \n",
    "        Nfeval += 1    \n",
    "        xIter.append(Xi)\n",
    "        \n",
    "    fprime = grad(func)\n",
    "    fhess = hessian(func)\n",
    "    \n",
    "    [xopt, fopt, func_calls, grad_calls, hess_calls,  warnflg] = \\\n",
    "        minimize.fmin_ncg(func, x0, fprime, callback=callbackF, full_output=True, disp=False, retall=False)\n",
    "    \n",
    "    return xopt, fopt, Nfeval, xIter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def armijo(function, xcur, searchdirection, c1, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f).p\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    numiter = 0\n",
    "    # check for Armijo condition\n",
    "    while ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))): \n",
    "        numiter += 1\n",
    "        alpha = alpha/2.0\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe(function, xcur, searchdirection, c1, c2, alpha0): \n",
    "    #f(x+alpha p) \\leq f(x) + c1 alpha grad(f)Tp\n",
    "    #grad(f(x+alpha p))Tp \\geq c2 grad(f)Tp\n",
    "    \n",
    "    alpha = alpha0\n",
    "    xnew = xcur + alpha * searchdirection\n",
    "    fcur = function(xcur)\n",
    "    fnew = function(xnew)\n",
    "    gradientCur = gradient_function(function,xcur)\n",
    "    gradientNew = gradient_function(function,xnew)\n",
    "    numiter = 0\n",
    "    \n",
    "    lb = 0\n",
    "    ub = np.inf \n",
    "    \n",
    "    # check for Wolfe conditions\n",
    "    # adapted from https://sites.math.washington.edu/~burke/crs/408/notes/nlp/line.pdf (pg. 8)\n",
    "    while True: \n",
    "        numiter += 1\n",
    "        if ((fnew) > (fcur + c1 * alpha * np.dot(gradientCur,searchdirection))):\n",
    "            ub = alpha\n",
    "            alpha = 0.5 * (lb + ub)\n",
    "        elif (np.dot(gradientNew,searchdirection) < c2 * np.dot(gradientCur,searchdirection)):\n",
    "        #elif (np.abs(np.dot(gradientNew,searchdirection)) > c2 * np.abs(np.dot(gradientCur,searchdirection))):\n",
    "            lb = alpha\n",
    "            if np.isinf(ub):\n",
    "                alpha = 2 * lb\n",
    "            else:\n",
    "                alpha = 0.5 * (lb + ub)\n",
    "        else:\n",
    "            break\n",
    "        xnew = xcur + alpha * searchdirection\n",
    "        fnew = function(xnew)\n",
    "        gradientNew = gradient_function(function,xnew)\n",
    "\n",
    "    return alpha, numiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calls Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point\n",
    "x0 = np.array([-2., 2.])\n",
    "\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "num_last_iters = 3\n",
    "xoptStar = [1.0, 1.0]  # optimal point\n",
    "\n",
    "# a: steepest_descent armijo\n",
    "# b: steepest_descent wolfe\n",
    "# c: steepest_descent constant\n",
    "# d: newton\n",
    "# e: ncg_scipy\n",
    "# f: cg_scipy\n",
    "# g: bfgs_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling all solvers\n",
    "xopt_a, fopt_a, it_a, xIter_a = steepest_descent(rosen, x0, 'armijo', c1, c2, alpha0)\n",
    "xopt_b, fopt_b, it_b, xIter_b = steepest_descent(rosen, x0, 'wolfe', c1, c2, alpha0)\n",
    "xopt_c, fopt_c, it_c, xIter_c = steepest_descent(rosen, x0, 'constant', c1, c2, alpha0)\n",
    "xopt_d, fopt_d, it_d, xIter_d = newton_descent(rosen, x0)\n",
    "xopt_e, fopt_e, it_e, xIter_e = ncg_scipy(rosen, x0)\n",
    "xopt_f, fopt_f, it_f, xIter_f = cg_scipy(rosen, x0)\n",
    "xopt_g, fopt_g, it_g, xIter_g = bfgs_scipy(rosen, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "# you can ignore this cell\n",
    "# here, we simply plot convergence on y-axis v/s the last four iterations of the solver\n",
    "diff_a = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_a]\n",
    "diff_b = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_b]\n",
    "diff_c = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_c]\n",
    "diff_d = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_d]\n",
    "diff_e = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_e]\n",
    "diff_f = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_f]\n",
    "diff_g = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_g]\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, figsize=(20,20))\n",
    "fig.text(0.5, 0.04, 'iteration',fontsize = 24, ha='center')\n",
    "fig.text(0.04, 0.5, '$||x^k - x^*||$',fontsize = 24, va='center', rotation='vertical')\n",
    "\n",
    "ax[0, 0].plot(list(range(it_a-num_last_iters, it_a+1)),diff_a[it_a-num_last_iters:it_a+1])\n",
    "ax[0, 1].plot(list(range(it_b-num_last_iters, it_b+1)),diff_b[it_b-num_last_iters:it_b+1])\n",
    "ax[1, 0].plot(list(range(it_c-num_last_iters, it_c+1)),diff_c[it_c-num_last_iters:it_c+1])\n",
    "ax[1, 1].plot(list(range(it_d-num_last_iters, it_d+1)),diff_d[it_d-num_last_iters:it_d+1])\n",
    "ax[2, 0].plot(list(range(it_e-num_last_iters, it_e+1)),diff_e[it_e-num_last_iters:it_e+1])\n",
    "ax[2, 1].plot(list(range(it_f-num_last_iters, it_f+1)),diff_f[it_f-num_last_iters:it_f+1])\n",
    "ax[3, 0].plot(list(range(it_g-num_last_iters, it_g+1)),diff_g[it_g-num_last_iters:it_g+1])\n",
    "ax[3, 1].axis('off')\n",
    "\n",
    "ax[0, 0].set_xticks(np.arange(it_a-num_last_iters, it_a+1, 1))\n",
    "ax[0, 1].set_xticks(np.arange(it_b-num_last_iters, it_b+1, 1))\n",
    "ax[1, 0].set_xticks(np.arange(it_c-num_last_iters, it_c+1, 1))\n",
    "ax[1, 1].set_xticks(np.arange(it_d-num_last_iters, it_d+1, 1))\n",
    "ax[2, 0].set_xticks(np.arange(it_e-num_last_iters, it_e+1, 1))\n",
    "ax[2, 1].set_xticks(np.arange(it_f-num_last_iters, it_f+1, 1))\n",
    "ax[3, 0].set_xticks(np.arange(it_g-num_last_iters, it_g+1, 1))\n",
    "\n",
    "ax[0, 0].title.set_text('steepest_descent armijo')\n",
    "ax[0, 1].title.set_text('steepest_descent wolfe')\n",
    "ax[1, 0].title.set_text('steepest_descent constant')\n",
    "ax[1, 1].title.set_text('newton descent')\n",
    "ax[2, 0].title.set_text('ncg_scipy')\n",
    "ax[2, 1].title.set_text('cg_scipy')\n",
    "ax[3, 0].title.set_text('bfgs_scipy')\n",
    "\n",
    "ax[0, 0].set_yscale('log')\n",
    "ax[0, 1].set_yscale('log')\n",
    "ax[1, 0].set_yscale('log')\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[2, 0].set_yscale('log')\n",
    "ax[2, 1].set_yscale('log')\n",
    "ax[3, 0].set_yscale('log')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjiman function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial point\n",
    "x0 = np.array([-2., 2.])\n",
    "\n",
    "c1 = 0.5 \n",
    "c2 = 0.6 \n",
    "alpha0 = 0.125 \n",
    "tol = 0.0001\n",
    "\n",
    "num_last_iters = 3\n",
    "xoptStar = np.array([-2.92293048, 2.04457258])  # approx optimum\n",
    "\n",
    "# a: steepest_descent armijo\n",
    "# b: steepest_descent wolfe\n",
    "# c: steepest_descent constant\n",
    "# d: newton\n",
    "# e: ncg_scipy\n",
    "# f: cg_scipy\n",
    "# g: bfgs_scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling all solvers\n",
    "xopt_a, fopt_a, it_a, xIter_a = steepest_descent(adjiman, x0, 'armijo', c1, c2, alpha0)\n",
    "xopt_b, fopt_b, it_b, xIter_b = steepest_descent(adjiman, x0, 'wolfe', c1, c2, alpha0)\n",
    "xopt_c, fopt_c, it_c, xIter_c = steepest_descent(adjiman, x0, 'constant', c1, c2, alpha0)\n",
    "xopt_d, fopt_d, it_d, xIter_d = newton_descent(adjiman, x0)\n",
    "xopt_e, fopt_e, it_e, xIter_e = ncg_scipy(adjiman, x0)\n",
    "xopt_f, fopt_f, it_f, xIter_f = cg_scipy(adjiman, x0)\n",
    "xopt_g, fopt_g, it_g, xIter_g = bfgs_scipy(adjiman, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "# you can ignore this cell\n",
    "# here, we simply plot convergence on y-axis v/s the last four iterations of the solver\n",
    "diff_a = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_a]\n",
    "diff_b = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_b]\n",
    "diff_c = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_c]\n",
    "diff_d = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_d]\n",
    "diff_e = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_e]\n",
    "diff_f = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_f]\n",
    "diff_g = [np.linalg.norm(iterate - xoptStar) for iterate in xIter_g]\n",
    "\n",
    "fig, ax = plt.subplots(4, 2,figsize=(20,20))\n",
    "fig.text(0.5, 0.04, 'iteration',fontsize = 24, ha='center')\n",
    "fig.text(0.04, 0.5, '$||x^k - x^*||$',fontsize = 24, va='center', rotation='vertical')\n",
    "\n",
    "ax[0, 0].plot(list(range(it_a-num_last_iters, it_a+1)),diff_a[it_a-num_last_iters:it_a+1])\n",
    "ax[0, 1].plot(list(range(it_b-num_last_iters, it_b+1)),diff_b[it_b-num_last_iters:it_b+1])\n",
    "ax[1, 0].plot(list(range(it_c-num_last_iters, it_c+1)),diff_c[it_c-num_last_iters:it_c+1])\n",
    "ax[1, 1].plot(list(range(it_d-num_last_iters, it_d+1)),diff_d[it_d-num_last_iters:it_d+1])\n",
    "ax[2, 0].plot(list(range(it_e-num_last_iters, it_e+1)),diff_e[it_e-num_last_iters:it_e+1])\n",
    "ax[2, 1].plot(list(range(it_f-num_last_iters, it_f+1)),diff_f[it_f-num_last_iters:it_f+1])\n",
    "ax[3, 0].plot(list(range(it_g-num_last_iters, it_g+1)),diff_g[it_g-num_last_iters:it_g+1])\n",
    "ax[3, 1].axis('off')\n",
    "\n",
    "ax[0, 0].set_xticks(np.arange(it_a-num_last_iters, it_a+1, 1))\n",
    "ax[0, 1].set_xticks(np.arange(it_b-num_last_iters, it_b+1, 1))\n",
    "ax[1, 0].set_xticks(np.arange(it_c-num_last_iters, it_c+1, 1))\n",
    "ax[1, 1].set_xticks(np.arange(it_d-num_last_iters, it_d+1, 1))\n",
    "ax[2, 0].set_xticks(np.arange(it_e-num_last_iters, it_e+1, 1))\n",
    "ax[2, 1].set_xticks(np.arange(it_f-num_last_iters, it_f+1, 1))\n",
    "ax[3, 0].set_xticks(np.arange(it_g-num_last_iters, it_g+1, 1))\n",
    "\n",
    "ax[0, 0].title.set_text('steepest_descent armijo')\n",
    "ax[0, 1].title.set_text('steepest_descent wolfe')\n",
    "ax[1, 0].title.set_text('steepest_descent constant')\n",
    "ax[1, 1].title.set_text('newton descent')\n",
    "ax[2, 0].title.set_text('ncg_scipy')\n",
    "ax[2, 1].title.set_text('cg_scipy')\n",
    "ax[3, 0].title.set_text('bfgs_scipy')\n",
    "\n",
    "ax[0, 0].set_yscale('log')\n",
    "ax[0, 1].set_yscale('log')\n",
    "ax[1, 0].set_yscale('log')\n",
    "ax[1, 1].set_yscale('log')\n",
    "ax[2, 0].set_yscale('log')\n",
    "ax[2, 1].set_yscale('log')\n",
    "ax[3, 0].set_yscale('log')\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
